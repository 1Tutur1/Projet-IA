--------Stockage de Arthur-------
import sys
import math
import pandas as pd
import pickle
from sklearn.preprocessing import OrdinalEncoder


def apprentissage_supervise(fichier_modele, fichier_scaler, fichier_encoder, features_csv):
    fichier_modele = open(fichier_modele, mode="rb")
    model = pickle.load(fichier_modele)
    fichier_scaler = open(fichier_scaler, mode="rb")
    scaler = pickle.load(fichier_scaler)
    fichier_encoder = open(fichier_encoder, mode="rb")
    encoder = pickle.load(fichier_encoder)
    features = pd.read_csv(features_csv, delimiter=';')
    colonnes_a_convertir = ['fk_stadedev', 'clc_quartier', 'clc_secteur']
    features[colonnes_a_convertir] = encoder.transform(features[colonnes_a_convertir])
    X = features[["haut_tot", "tronc_diam", "clc_quartier", "clc_secteur", "fk_stadedev"]]
    X_fitted = scaler.transform(X)
    predictions = model.predict(X_fitted)
    print(X)
    print(predictions)


apprentissage_supervise("model.pkl", "f2_scaler.pkl", "f2_encoder.pkl", "test_script2.csv")

## fonctionnalité 2 ##

import pandas as pd
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score
import time
import pickle

# Charger le fichier CSV
bdd = pd.read_csv('C:/Users/apeta/PycharmProjects/projetIA3A/Data_Arbre.csv')

# Se débarrasser des colonnes inutiles
bdd.drop(columns=["remarquable", "feuillage", "villeca", "fk_nomtech", "clc_nbr_diag",
                  "fk_prec_estim", "fk_revetement", "fk_situation", "fk_pied", "fk_port",
                  "fk_arb_etat"], inplace=True)

# Convertir les données non-numériques en numérique, parmi celles dont on va se servir
colonnes_a_convertir = ['fk_stadedev', 'clc_quartier', 'clc_secteur']
encoder = OrdinalEncoder()
bdd[colonnes_a_convertir] = encoder.fit_transform(bdd[colonnes_a_convertir])
f2_encoder = open(file="f2_encoder.pkl", mode="wb")
pickle.dump(encoder, f2_encoder)
# Fin conversion
debut = time.time()

# Bouclage afin de ranger les âges par classes
for i in range(len(bdd)):
    if bdd["age_estim"][i] < 5:
        bdd.loc[i, "age_estim"] = 0
    elif 5 <= bdd["age_estim"][i] < 10:
        bdd.loc[i, "age_estim"] = 1
    elif 10 <= bdd["age_estim"][i] < 20:
        bdd.loc[i, "age_estim"] = 2
    elif 20 <= bdd["age_estim"][i] < 30:
        bdd.loc[i, "age_estim"] = 3
    elif 30 <= bdd["age_estim"][i] < 40:
        bdd.loc[i, "age_estim"] = 4
    elif 40 <= bdd["age_estim"][i] < 60:
        bdd.loc[i, "age_estim"] = 5
    elif 60 <= bdd["age_estim"][i] < 80:
        bdd.loc[i, "age_estim"] = 6
    elif 80 <= bdd["age_estim"][i] < 100:
        bdd.loc[i, "age_estim"] = 7
    else:
        bdd.loc[i, "age_estim"] = 8

bdd = bdd.astype({"age_estim": int})
# Création des bases X et Y qui contiennent les données intéressantes
X = bdd[["haut_tot", "tronc_diam", "clc_quartier", "clc_secteur", "fk_stadedev"]]
y = bdd["age_estim"]

# Diviser les données en bases d'apprentissage et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normaliser les données
scaler = StandardScaler()
f2_scaler = open(file="f2_scaler.pkl", mode="wb")
scaler.fit(X)
pickle.dump(scaler, f2_scaler)
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Création et entraînement du modèle
model = RandomForestClassifier(random_state=42)
fitted_model = model.fit(X_train, y_train)
fichier_modele = open(file="model.pkl", mode="wb")
pickle.dump(fitted_model, fichier_modele)
param_grid = {'n_estimators': [50, 100, 200], 'max_features': [2, 3, 4, 5]}

search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
search.fit(X_train, y_train)

'''fonction qui utilise une méthode spécifique'''
def train_model(X_train, y_train, method):
    if method == "random_forest":
        model = RandomForestClassifier(random_state=42)
    elif method == "decision_tree":
        model = DecisionTreeClassifier(random_state=42)

    else:
        raise ValueError("Méthode de classification non supportée")

    model.fit(X_train, y_train)
    return model


# Prédire les valeurs de test
y_pred = search.best_estimator_.predict(X_test)

# Tester les métriques
# Note : l'average à weighted n'est pas utile en classification binaire.
taux_classification = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
matrice_confusion = confusion_matrix(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
fin = time.time()
print(f'Meilleurs paramètres : {search.best_params_}')
print(f'Taux de classification : {taux_classification}')
print(f'Précision : {precision}')
print(f'Rappel : {recall}')
print(f"f1_score :{f1}")
print("Matrice de confusion:")
print(matrice_confusion)
print(f'Temps de calcul : {fin - debut}')

## Fonctionnalité 2bis ##

import pandas as pd
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score
import time
import pickle

# Charger le fichier CSV
bdd = pd.read_csv('C:/Users/apeta/PycharmProjects/projetIA3A/Data_Arbre.csv', delimiter=';')

# Se débarrasser des colonnes inutiles
bdd.drop(columns=["remarquable", "villeca", "clc_nbr_diag",
                  "fk_prec_estim", "fk_port", "clc_quartier", "clc_secteur"
                  ], inplace=True)

# Convertir les données non-numériques en numérique, parmi celles dont on va se servir
colonnes_a_convertir = ['fk_arb_etat', 'feuillage', 'fk_stadedev', 'fk_pied', 'fk_situation', 'fk_revetement',
                        'fk_nomtech']
encoder = OrdinalEncoder()
bdd[colonnes_a_convertir] = encoder.fit_transform(bdd[colonnes_a_convertir])
f3_encoder = open(file="f3_encoder.pkl", mode="wb")
pickle.dump(encoder, f3_encoder)

# Fin conversion
bdd = bdd.astype({"fk_arb_etat": int})
X = bdd[["haut_tot", "haut_tronc", "tronc_diam", "fk_stadedev", "fk_pied", "fk_situation", "fk_revetement", "age_estim",
         "fk_nomtech", "feuillage"]]
y = bdd["fk_arb_etat"]

# Diviser les données en bases d'apprentissage et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normaliser les données
scaler = StandardScaler()
f3_scaler = open(file="f3_scaler.pkl", mode="wb")
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
pickle.dump(scaler, f3_scaler)

debut = time.time()

model = DecisionTreeClassifier(random_state=42)
fitted_model = model.fit(X_train, y_train)
fichier_modele = open(file="modelf2bis.pkl", mode="wb")
pickle.dump(fitted_model, fichier_modele)
param_grid = {'max_features': [2, 3, 4, 5, 6, 7, 8, 9, 10]}
search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
search.fit(X_train, y_train)
y_pred = search.best_estimator_.predict(X_test)
print(y_pred)
# Tester les métriques
taux_classification = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='micro', zero_division=0)
recall = recall_score(y_test, y_pred, average='micro')
matrice_confusion = confusion_matrix(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='micro')
fin = time.time()
print(f'Meilleurs paramètres : {search.best_params_}')
print(f'Taux de classification : {taux_classification}')
print(f'Précision : {precision}')
print(f'Rappel : {recall}')
print(f"f1_score :{f1}")
print("Matrice de confusion:")
print(matrice_confusion)
print(f'Temps de calcul : {fin - debut}')


--------Stockage de pa----

import pandas as pd
import numpy as np
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.cluster import KMeans
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import csv
import pickle

bdd = pd.read_csv("../Data_Arbre.csv", delimiter=";")

def cluster_taille(nb_cluster):

    bdd.drop(columns=["clc_quartier","clc_secteur","feuillage","villeca","fk_nomtech","clc_nbr_diag","fk_prec_estim","fk_revetement","fk_pied","fk_port","fk_stadedev","fk_arb_etat","remarquable"], inplace=True)

    colonnes_a_convertir = ['fk_situation']
    encoder = OrdinalEncoder()
    bdd[colonnes_a_convertir] = encoder.fit_transform(bdd[colonnes_a_convertir])

    f1_encoder = open(file="../Fonctionnalite_4/script_fonction_1/encoder_1.pkl", mode="wb")
    pickle.dump(encoder, f1_encoder)

    X = bdd[["haut_tot", "haut_tronc","tronc_diam", "fk_situation","age_estim"]]

    obj_kmeans = KMeans(n_clusters=nb_cluster, random_state=42)
    obj_kmeans.fit(X)

    scaler = StandardScaler()
    f1_scaler = open(file="../Fonctionnalite_4/script_fonction_1/scaler_1.pkl", mode="wb")
    pickle.dump(scaler, f1_scaler)
    scaler.fit(X)

    indices_cluster = obj_kmeans.predict(X)

    #crée le fichier contenant les centroides nécessaires pour les scripts de la partie 3
    centroids = obj_kmeans.cluster_centers_
    fichier_centroide = open(file="../Fonctionnalite_4/script_fonction_1/fichiers_centroides/centroides_KMeans_"+str(nb_cluster)+".csv", mode='w',newline='')
    ecriture = csv.writer(fichier_centroide, delimiter=';')
    ecriture.writerow(["haut_tot", "haut_tronc", "tronc_diam", "fk_situation", "age_estim"])
    for centroide in centroids:
        ecriture.writerow(centroide)
    fichier_centroide.close()
    return indices_cluster

def visualisation_cluster_taille(nb_cluster):
    indices_cluster = cluster_taille(nb_cluster)
    bdd["cluster"] = indices_cluster
    stquentin = mpimg.imread("stquentin.PNG")
    bdd.plot(kind="scatter", x="longitude", y="latitude", figsize=(10, 7), s=10, colorbar=False)

    colors = {0: 'darkgreen', 1: 'lime', 2: 'yellow', 3: 'orange', 4: 'red', 5: 'purple', 6: 'indigo', 7: 'royalblue',8: 'lightskyblue'}

    for cluster, color in colors.items():
        cluster_a_placer = bdd[bdd["cluster"] == cluster]
        if len(cluster_a_placer) != 0:
            plt.scatter(cluster_a_placer["longitude"], cluster_a_placer["latitude"], label="Cluster : " + str(cluster+1),
                        color=color, s=5)

    plt.imshow(stquentin, extent=(3.24, 3.33, 49.825, 49.873))
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.legend()
    plt.show()



def visualisation_cluster_taille(nb_cluster):
    indices_cluster = cluster_taille(nb_cluster)
    bdd["cluster"] = indices_cluster
    stquentin = mpimg.imread("C:/Users/pierr/PycharmProjects/Projet_IA/stquentin.PNG")
    bdd.plot(kind="scatter", x="longitude", y="latitude",c="cluster",colormap="gist_rainbow", figsize=(10,7),s=10,colorbar=False)
    plt.imshow(stquentin, extent=(3.24, 3.33, 49.825, 49.873))
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.show()

visualisation_cluster_taille(3)

import pandas as pd
import numpy as np
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.cluster import Birch
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import csv
import pickle

bdd = pd.read_csv("../Data_Arbre.csv", delimiter=";")

def birch(treshold):

    bdd.drop(columns=["clc_quartier","clc_secteur","feuillage","villeca","fk_nomtech","clc_nbr_diag","fk_prec_estim","fk_revetement","fk_pied","fk_port","fk_stadedev","fk_arb_etat","remarquable"], inplace=True)

    colonnes_a_convertir = ['fk_situation']
    encoder = OrdinalEncoder()
    bdd[colonnes_a_convertir] = encoder.fit_transform(bdd[colonnes_a_convertir])

    f1_encoder = open(file="../Fonctionnalite_4/script_fonction_1/encoder_1.pkl", mode="wb")
    pickle.dump(encoder, f1_encoder)

    X = bdd[["haut_tot", "haut_tronc","tronc_diam", "fk_situation","age_estim"]]

    scaler = StandardScaler()
    f1_scaler = open(file="../Fonctionnalite_4/script_fonction_1/scaler_1.pkl", mode="wb")
    pickle.dump(scaler, f1_scaler)
    scaler.fit(X)

    model_birch = Birch(threshold = treshold)
    model_birch.fit(X)
    fichier_model= open(file="../Fonctionnalite_4/script_fonction_1/models/model_1_Birch.pkl", mode="wb")
    pickle.dump(model_birch, fichier_model)
    bdd["cluster"] = model_birch.predict(X)

    stquentin = mpimg.imread("stquentin.PNG")
    bdd.plot(kind="scatter", x="longitude", y="latitude", figsize=(10,7),s=10,colorbar=False)
    colors = {0: 'darkgreen', 1: 'lime', 2: 'yellow', 3: 'orange', 4: 'red', 5:'purple', 6:'indigo', 7:'royablue', 8:'lightskyblue'}

    for cluster, color in colors.items():
        cluster_a_placer = bdd[bdd["cluster"] == cluster]
        if len(cluster_a_placer)!=0:
            plt.scatter(cluster_a_placer["longitude"], cluster_a_placer["latitude"], label="Cluster : " +str(cluster+1), color=color,s=5)

    plt.imshow(stquentin, extent=(3.24, 3.33, 49.825, 49.873))
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.legend()
    plt.show()

birch(0.4)

import sys
import math
import pandas as pd
import pickle

 !! SCRIPT 1 !!
      |
     \ /
      
def apprentisage_non_supervise(features_csv,nb_cluster,methode):
    fichier_encoder = open("Fonctionnalite_4/script_fonction_1/encoder_1.pkl", mode="rb")
    encoder = pickle.load(fichier_encoder)

    features = pd.read_csv(features_csv, delimiter=';')
    colonnes_a_convertir = ['fk_situation']
    features[colonnes_a_convertir] = encoder.transform(features[colonnes_a_convertir])

    if methode == "KMeans":
        fichier_centroide = pd.read_csv("Fonctionnalite_4/script_fonction_1/fichiers_centroides/centroides_KMeans_"+str(nb_cluster)+".csv", delimiter=";")
        liste_distance = []
        for i in range(0, len(fichier_centroide)):
            distance = math.sqrt((fichier_centroide["haut_tot"][i] - features["haut_tot"]) ** 2
                                 + (fichier_centroide["haut_tronc"][i] - features["haut_tronc"]) ** 2
                                 + (fichier_centroide["tronc_diam"][i] - features["tronc_diam"]) ** 2
                                 + (fichier_centroide["fk_situation"][i] - features["fk_situation"]) ** 2
                                 + (fichier_centroide["age_estim"][i] - features["age_estim"]) ** 2)
            liste_distance.append(distance)
        indice_min = liste_distance.index(min(liste_distance))
        return f"L'arbre appartient au cluster {indice_min}"

    elif methode == "Birch":
        fichier_modele =  open("Fonctionnalite_4/script_fonction_1/models/model_1_Birch.pkl", mode="rb")
        modele = pickle.load(fichier_modele)
        prediction = modele.predict(features)
        return f"L'arbre appartient au cluster {int(prediction)}"
    else :
        return "Méthode spécifiée incorrecte"


commande = apprentisage_non_supervise(sys.argv[1],sys.argv[2],sys.argv[3])
print(commande)

#Pour KMeans et 3 clusters : C:\Users\pierr\AppData\Local\Programs\Python\Python39\python.exe Fonctionnalite_4\script_fonction_1\script_main_1.py Test_script_1.csv 3 KMeans

#Pour Birch : C:\Users\pierr\AppData\Local\Programs\Python\Python39\python.exe Fonctionnalite_4\script_fonction_1\script_main_1.py Test_script_1.csv False Birch







import sys
import math
import pandas as pd

def apprentisage_non_supervise(fichier_centroide,features):
    centroides = pd.read_csv(fichier_centroide,delimiter = ";")
    liste_distance = []
    liste_features = features.split(",")
    for i in range(0,len(centroides)):
        distance = math.sqrt((centroides["haut_tot"][i]-float(liste_features[0]))**2
                             +(centroides["haut_tronc"][i]-float(liste_features[1]))**2
                             +(centroides["tronc_diam"][i]-float(liste_features[2])) ** 2
                             +(centroides["fk_situation"][i]-float(liste_features[3])) ** 2
                             +(centroides["age_estim"][i]-float(liste_features[4])) ** 2)
        liste_distance.append(distance)
    indice_min = liste_distance.index(min(liste_distance))
    return f"L'arbre appartient au cluster {indice_min}"
