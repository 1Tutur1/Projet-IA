--------Stockage de Arthur-------

import pandas as pd
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score
import time
import pickle

# Charger le fichier CSV
bdd = pd.read_csv('C:/Users/apeta/PycharmProjects/projetIA3A/Data_Arbre.csv')

# Se débarrasser des colonnes inutiles
bdd.drop(columns=["remarquable", "feuillage", "villeca", "fk_nomtech", "clc_nbr_diag",
                  "fk_prec_estim", "fk_revetement", "fk_situation", "fk_pied", "fk_port",
                  "fk_arb_etat"], inplace=True)

# Convertir les données non-numériques en numérique, parmi celles dont on va se servir
colonnes_a_convertir = ['fk_stadedev', 'clc_quartier', 'clc_secteur']
encoder = OrdinalEncoder()
bdd[colonnes_a_convertir] = encoder.fit_transform(bdd[colonnes_a_convertir])
f2_encoder = open(file="f2_encoder.pkl", mode="wb")
pickle.dump(bdd[colonnes_a_convertir], f2_encoder)
# Fin conversion
debut = time.time()

# Bouclage afin de ranger les âges par classes
for i in range(len(bdd)):
    if bdd["age_estim"][i] < 5:
        bdd.loc[i, "age_estim"] = 0
    elif 5 <= bdd["age_estim"][i] < 10:
        bdd.loc[i, "age_estim"] = 1
    elif 10 <= bdd["age_estim"][i] < 20:
        bdd.loc[i, "age_estim"] = 2
    elif 20 <= bdd["age_estim"][i] < 30:
        bdd.loc[i, "age_estim"] = 3
    elif 30 <= bdd["age_estim"][i] < 40:
        bdd.loc[i, "age_estim"] = 4
    elif 40 <= bdd["age_estim"][i] < 60:
        bdd.loc[i, "age_estim"] = 5
    elif 60 <= bdd["age_estim"][i] < 80:
        bdd.loc[i, "age_estim"] = 6
    elif 80 <= bdd["age_estim"][i] < 100:
        bdd.loc[i, "age_estim"] = 7
    else:
        bdd.loc[i, "age_estim"] = 8

bdd = bdd.astype({"age_estim": int})
# Création des bases X et Y qui contiennent les données intéressantes
X = bdd[["haut_tot", "tronc_diam", "clc_quartier", "clc_secteur", "fk_stadedev"]]
y = bdd["age_estim"]

# Diviser les données en bases d'apprentissage et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normaliser les données
scaler = StandardScaler()
f2_scaler = open(file="f2_scaler.pkl", mode="wb")
pickle.dump(scaler, f2_scaler)
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Création et entraînement du modèle
model = RandomForestClassifier(random_state=42)
fitted_model = model.fit(X_train, y_train)
fichier_modele = open(file="model.pkl", mode="wb")
pickle.dump(fitted_model, fichier_modele)
param_grid = {'n_estimators': [50, 100, 200], 'max_features': [2, 3, 4, 5]}

search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
search.fit(X_train, y_train)

# Prédire les valeurs de test
y_pred = search.best_estimator_.predict(X_test)

# Tester les métriques
# Note : l'average à weighted n'est pas utile en classification binaire.
taux_classification = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
matrice_confusion = confusion_matrix(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
fin = time.time()
print(f'Meilleurs paramètres : {search.best_params_}')
print(f'Taux de classification : {taux_classification}')
print(f'Précision : {precision}')
print(f'Rappel : {recall}')
print(f"f1_score :{f1}")
print("Matrice de confusion:")
print(matrice_confusion)
print(f'Temps de calcul : {fin - debut}')


--------Stockage de pa----

import pandas as pd
import numpy as np
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.cluster import KMeans
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import csv

bdd = pd.read_csv("Data_Arbre.csv",delimiter=";")

def cluster_taille(nb_cluster):

    bdd.drop(columns=["clc_quartier","clc_secteur","feuillage","villeca","fk_nomtech","clc_nbr_diag","fk_prec_estim","fk_revetement","fk_pied","fk_port","fk_stadedev","fk_arb_etat","remarquable"], inplace=True)

    colonnes_a_convertir = ['fk_situation']
    encoder = OrdinalEncoder()
    bdd[colonnes_a_convertir] = encoder.fit_transform(bdd[colonnes_a_convertir])
    X = bdd[["haut_tot", "haut_tronc","tronc_diam", "fk_situation","age_estim"]]

    obj_kmeans = KMeans(n_clusters=nb_cluster, random_state=42)
    obj_kmeans.fit(X)
    indices_cluster = obj_kmeans.predict(X)

    #crée le fichier contenant les centroides nécessaires pour les scripts de la partie 3
    centroids = obj_kmeans.cluster_centers_
    fichier_centroide = open(file="centroides"+str(nb_cluster)+".csv", mode='w',newline='')
    ecriture = csv.writer(fichier_centroide, delimiter=';')
    ecriture.writerow(["haut_tot", "haut_tronc", "tronc_diam", "fk_situation", "age_estim"])
    for centroide in centroids:
        ecriture.writerow(centroide)
    fichier_centroide.close()
    return indices_cluster

def visualisation_cluster_taille(nb_cluster):
    indices_cluster = cluster_taille(nb_cluster)
    bdd["cluster"] = indices_cluster
    stquentin = mpimg.imread("C:/Users/pierr/PycharmProjects/Projet_IA/stquentin.PNG")
    bdd.plot(kind="scatter", x="longitude", y="latitude",c="cluster",colormap="gist_rainbow", figsize=(10,7),s=10,colorbar=False)
    plt.imshow(stquentin, extent=(3.24, 3.33, 49.825, 49.873))
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.show()

visualisation_cluster_taille(3)

import sys
import math
import pandas as pd

def apprentisage_non_supervise(fichier_centroide,features):
    centroides = pd.read_csv(fichier_centroide,delimiter = ";")
    liste_distance = []
    liste_features = features.split(",")
    for i in range(0,len(centroides)):
        distance = math.sqrt((centroides["haut_tot"][i]-float(liste_features[0]))**2
                             +(centroides["haut_tronc"][i]-float(liste_features[1]))**2
                             +(centroides["tronc_diam"][i]-float(liste_features[2])) ** 2
                             +(centroides["fk_situation"][i]-float(liste_features[3])) ** 2
                             +(centroides["age_estim"][i]-float(liste_features[4])) ** 2)
        liste_distance.append(distance)
    indice_min = liste_distance.index(min(liste_distance))
    return f"L'arbre appartient au cluster {indice_min}"
