## Afficher les arbres en fonction de leur hauteur sur la carte ##
stquentin_img = mpimg.imread('C:/Users/apeta/PycharmProjects/projetIA3A/stquentin.PNG')

# Tracer le scatter plot
bdd.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4, label="arbres",
         c="haut_tot", colormap="gist_rainbow", colorbar=True, sharex=False, figsize=(10,7),s=10)
# Ajouter l'image avec plt.imshow
plt.imshow(stquentin_img, extent=(3.24, 3.33, 49.825, 49.873), alpha=0.5)

# Afficher la légende et le graphique
plt.legend()
plt.show()

## Fin affichage sur la carte


--------PA-----------

bdd = pd.read_csv("C:/Users/pierr/PycharmProjects/Projet_IA/Data_Arbre.csv",delimiter=";")

def cluster_hauteur(nb_cluster):
    bdd.drop(columns=["clc_quartier","clc_secteur","remarquable","feuillage","villeca","fk_nomtech","clc_nbr_diag","fk_prec_estim","age_estim","fk_revetement","fk_situation","fk_pied","fk_port","fk_stadedev","fk_arb_etat","tronc_diam","haut_tronc"], inplace=True)
    X = bdd[["haut_tot"]]

    obj_kmeans = KMeans(n_clusters=nb_cluster, random_state=42)
    obj_kmeans.fit(X)
    indices_cluster = obj_kmeans.predict(X)
    return indices_cluster


def visualisation_cluster_hauteur(nb_cluster):
    indices_cluster = cluster_hauteur(nb_cluster)
    bdd["cluster"] = indices_cluster
    stquentin = mpimg.imread("C:/Users/pierr/PycharmProjects/Projet_IA/stquentin.PNG")
    bdd.plot(kind="scatter", x="longitude", y="latitude", label="arbres",c="cluster",cmap="rainbow", figsize=(10,7),s=10,colorbar=False)
    plt.imshow(stquentin, extent=(3.24, 3.33, 49.825, 49.873))

    plt.show()

visualisation_cluster_hauteur(5)

-------------------------------------------
import pandas as pd
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score
import time

# Charger le fichier CSV
bdd = pd.read_csv('C:/Users/apeta/PycharmProjects/projetIA3A/Data_Arbre.csv')

# Se débarrasser des colonnes inutiles
bdd.drop(columns=["remarquable", "feuillage", "villeca", "fk_nomtech", "clc_nbr_diag",
                  "fk_prec_estim", "fk_revetement", "fk_situation", "fk_pied", "fk_port",
                  "fk_arb_etat"], inplace=True)

# Convertir les données non-numériques en numérique, parmi celles dont on va se servir
colonnes_a_convertir = ['fk_stadedev', 'clc_quartier', 'clc_secteur']
encoder = OrdinalEncoder()
bdd[colonnes_a_convertir] = encoder.fit_transform(bdd[colonnes_a_convertir])

# Fin conversion
debut = time.time()

#Bouclage afin de ranger les âges par classes
for i in range(len(bdd)):
    if bdd["age_estim"][i] < 5:
        bdd.loc[i, "age_estim"] = 0
    elif 5 <= bdd["age_estim"][i] < 10:
        bdd.loc[i, "age_estim"] = 1
    elif 10 <= bdd["age_estim"][i] < 20:
        bdd.loc[i, "age_estim"] = 2
    elif 20 <= bdd["age_estim"][i] < 30:
        bdd.loc[i, "age_estim"] = 3
    elif 30 <= bdd["age_estim"][i] < 40:
        bdd.loc[i, "age_estim"] = 4
    elif 40 <= bdd["age_estim"][i] < 60:
        bdd.loc[i, "age_estim"] = 5
    elif 60 <= bdd["age_estim"][i] < 80:
        bdd.loc[i, "age_estim"] = 6
    elif 80 <= bdd["age_estim"][i] < 100:
        bdd.loc[i, "age_estim"] = 7
    else:
        bdd.loc[i, "age_estim"] = 8

bdd = bdd.astype({"age_estim": int})
# Création des bases X et Y qui contiennent les données intéressantes
X = bdd[["haut_tot", "tronc_diam", "clc_quartier", "clc_secteur", "fk_stadedev"]]
y = bdd["age_estim"]

# Diviser les données en bases d'apprentissage et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normaliser les données
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Création et entraînement du modèle
model = KNeighborsClassifier()
model.fit(X_train, y_train)
param_grid = {'n_neighbors':[3,5,7,9]}

search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
search.fit(X_train, y_train)

# Prédire les valeurs de test
y_pred = search.best_estimator_.predict(X_test)

# Tester les métriques
taux_classification = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
matrice_confusion = confusion_matrix(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
fin = time.time()
print(f'Meilleurs paramètres : {search.best_params_}')
print(f'Taux de classification : {taux_classification}')
print(f'Précision : {precision}')
print(f'Rappel : {recall}')
print(f"f1_score :{f1}")
print("Matrice de confusion:")
print(matrice_confusion)
print(f'Temps de calcul : {fin - debut}')


